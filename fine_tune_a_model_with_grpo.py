# -*- coding: utf-8 -*-
"""Fine_tune_a_model_with_GRPO (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uB5TR3hxRN0YzI67avIm9hLy6UQxmG1z

# Install dependencies
"""

!pip install unsloth vllm
!pip install --upgrade pillow

"""Now we’ll import the necessary libraries.


"""

import torch
import re
from datasets import load_dataset
from trl import GRPOConfig, GRPOTrainer
from unsloth import FastLanguageModel

"""#Load the dataset
Now, let’s load the dataset. In this case, we’ll use the mlabonne/smoltldr dataset, which contains a list of short stories.
"""

dataset = load_dataset("mlabonne/smoltldr")
print(dataset)

dataset['train'][10]['prompt']

"""#Load model
We’ll use the SmolLM2-135M model.
"""

model_id = "HuggingFaceTB/SmolLM-135M-Instruct"
max_seq_length = 1024  # Can increase for longer reasoning traces
lora_rank = 32  # Larger rank = smarter, but slower

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name=model_id,
    max_seq_length=max_seq_length,
    load_in_4bit=True,  # False for LoRA 16bit
    fast_inference=True,  # Enable vLLM fast inference
    max_lora_rank=lora_rank,
    gpu_memory_utilization=0.6,  # Reduce if out of memory
)

"""#output from model
Running the model on the first 5 prompts.
"""

for i in range(5):

  # Define input prompt
  input_text = dataset['train'][i]['prompt']
  # Convert input text into input format for the model, load it on GPU
  inputs = tokenizer(input_text, return_tensors="pt").to("cuda")  # Ensure the model and tensors are on the GPU


  # Generate output tokens
  output_tokens = model.generate(**inputs, max_length=500)  # Adjust max_length as needed

  # Slice to get only the generated tokens (excluding input tokens)
  generated_tokens = output_tokens[0][inputs['input_ids'].shape[1]:]  # Remove input tokens

  # Decode only the generated tokens
  output_text = tokenizer.decode(generated_tokens, skip_special_tokens=True)

  # Print the generated response
  print(f"------------ response to {i+1} prompt------------ ")
  print(output_text)

# Load LoRA

model = FastLanguageModel.get_peft_model(
    model,
    r=lora_rank,  # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128
    target_modules=[
        "q_proj",
        "k_proj",
        "v_proj",
        "o_proj",
        "gate_proj",
        "up_proj",
        "down_proj",
    ],  # Remove QKVO if out of memory
    lora_alpha=lora_rank,
    use_gradient_checkpointing="unsloth",  # Enable long context finetuning
    random_state=3407,
)

"""# Define the reward function
GRPO can use any reward function to improve the model. In this case, we’ll use a simple reward function that encourages the model to generate text that is 50 tokens long.
"""

# Reward function
ideal_length = 50


def reward_len(completions, **kwargs):
    return [-abs(ideal_length - len(completion)) for completion in completions]

"""#Define the training arguments"""

# Training arguments
max_prompt_length = 256
training_args = GRPOConfig(
    report_to="none",  # Can use Weights & Biases
    output_dir="outputs",
    learning_rate=2e-5,
    per_device_train_batch_size=8,
    gradient_accumulation_steps=2,
    max_prompt_length=512,
    max_completion_length=96,
    num_generations=8,
    optim="adamw_8bit",
    num_train_epochs=1,
    logging_steps=5,
)

"""#Train model"""

trainer = GRPOTrainer(
    model=model,
    processing_class=tokenizer,
    reward_funcs=[
        reward_len,
    ],
    args=training_args,
    train_dataset=dataset['train'],
)

trainer.train()

